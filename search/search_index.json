{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#optialgo-and-scikit-learn","title":"<code>optialgo</code> and <code>scikit-learn</code>","text":"<p>If you are used to modeling with <code>scikit-learn</code>, congratulations \ud83e\udd29 you will find optialgo easy to use!</p>"},{"location":"#main-features","title":"Main Features","text":"<pre><code>1. Data Prepration\n2. Data Preprocessing\n3. Text Preprocessing (2x Faster)\n3. Comparing Model\n4. Set Model\n5. Prediction\n6. HyperParameter Tuning\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>Before installing OptiAlgo, it is recommended to create an environment first.</p> <p><pre><code>pip install optialgo\n</code></pre> or</p> <pre><code>pip install git+https://github.com/nsandarma/OptiAlgo.git\n</code></pre> <p>and for text preprocessing</p> <pre><code>&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; nltk.download('all')\n</code></pre>"},{"location":"#overview","title":"Overview","text":"<pre><code>import pandas as pd\nfrom optialgo import Dataset, Classification\n\ndf = pd.read_csv('dataset_ex/drug200.csv')\nfeatures = ['Age','Sex','BP','Cholesterol',\"Na_to_K\"]\ntarget = 'Drug'\n\ndataset = Dataset(dataframe=df)\ndataset.fit(features=features,target=target)\n\nclf = Classification()\nresult = clf.compare_model(output='table',train_val=True)\nprint(result)\n</code></pre>"},{"location":"abc/","title":"Abstract Class","text":""},{"location":"abc/#optialgo.Parent","title":"Parent","text":"<pre><code>Parent(dataset: Dataset, algorithm: str = None)\n</code></pre> <p>               Bases: <code>ABC</code></p>"},{"location":"abc/#optialgo.Parent.dataset","title":"dataset  <code>property</code>","text":"<pre><code>dataset\n</code></pre> <p>Gets the dataset.</p> <p>Returns:</p> <ul> <li>           \u2013            <p>The dataset.</p> </li> </ul>"},{"location":"abc/#optialgo.Parent.get_list_models","title":"get_list_models  <code>property</code>","text":"<pre><code>get_list_models\n</code></pre> <p>Gets the list of models.</p> <p>Returns:</p> <ul> <li> <code>list</code>          \u2013            <p>A list of model names.</p> </li> </ul>"},{"location":"abc/#optialgo.Parent.get_metrics","title":"get_metrics  <code>property</code>","text":"<pre><code>get_metrics\n</code></pre> <p>Gets the metrics.</p> <p>Returns:</p> <ul> <li> <code>list</code>          \u2013            <p>A list of metrics.</p> </li> </ul>"},{"location":"abc/#optialgo.Parent.get_params_from_model","title":"get_params_from_model  <code>property</code>","text":"<pre><code>get_params_from_model\n</code></pre> <p>Gets parameters from the model.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the model is not found.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>Parameters of the model.</p> </li> </ul>"},{"location":"abc/#optialgo.Parent.get_result_compare_models","title":"get_result_compare_models  <code>property</code>","text":"<pre><code>get_result_compare_models\n</code></pre> <p>Gets the result of model comparison.</p> <p>Raises:</p> <ul> <li> <code>AttributeError</code>             \u2013            <p>If the result_compare_models attribute is not found.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>The result of model comparison.</p> </li> </ul>"},{"location":"abc/#optialgo.Parent.model","title":"model  <code>property</code>","text":"<pre><code>model\n</code></pre> <p>Gets the model.</p> <p>Returns:</p> <ul> <li>           \u2013            <p>The model.</p> </li> </ul>"},{"location":"abc/#optialgo.Parent.find_best_params","title":"find_best_params","text":"<pre><code>find_best_params(param_grid: dict, inplace=False)\n</code></pre> <p>Perform hyperparameter tuning to find the best parameters for the model.</p> <p>This method uses grid search cross-validation to find the best parameters for the model stored in the <code>self.model</code> attribute. It takes a dictionary of parameter values to search over and returns the best score and parameters found. Optionally, it can set the best parameters to the model in place.</p> <p>Parameters:</p> <ul> <li> <code>param_grid</code>           \u2013            <p>A dictionary where the keys are parameter names and the values are lists of parameter settings to try. This dictionary is used for performing grid search.</p> </li> <li> <code>inplace</code>           \u2013            <p>If True, the best parameters found by grid search will be set to the model in place. Default is False.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple or None: If <code>inplace</code> is False, returns a tuple containing the best score and the best parameters found by grid search. If <code>inplace</code> is True, the method sets the best parameters to the model and returns None.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the <code>self.model</code> attribute is not found.</p> </li> </ul> <p>Examples: <pre><code>reg = Regression(dataset, algorithm='linear_regression') # or Classification\nparam_grid = {'alpha': [0.1, 0.01, 0.001], 'max_iter': [100, 1000, 10000]}\nbest_score, best_params = reg.find_best_params(param_grid)\nprint(\"Best Score:\", best_score)\nprint(\"Best Parameters:\", best_params)\n</code></pre></p>"},{"location":"abc/#optialgo.Parent.predict","title":"predict","text":"<pre><code>predict(X_test: ndarray, transform: bool = True)\n</code></pre> <p>Predict for the given test data.</p> <p>Parameters:</p> <ul> <li> <code>X_test</code>           \u2013            <p>The test data to predict labels for.</p> </li> <li> <code>transform</code>           \u2013            <p>Whether to transform the test data using <code>flow_from_array</code> method before prediction (default is True).</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>The predicted for the test data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the model is not found.</p> </li> </ul>"},{"location":"abc/#optialgo.Parent.save","title":"save","text":"<pre><code>save()\n</code></pre> <p>Serialize and save the model object using pickle.</p> <p>Returns:</p> <ul> <li> <code>bytes</code>          \u2013            <p>Serialized representation of the optialgo object.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PickleError</code>             \u2013            <p>If serialization fails.</p> </li> </ul>"},{"location":"abc/#optialgo.Parent.set_model","title":"set_model","text":"<pre><code>set_model(algorithm: str)\n</code></pre> <p>Determine the algorithm to use</p>"},{"location":"abc/#optialgo.Parent.set_params","title":"set_params","text":"<pre><code>set_params(params: dict) -&gt; None\n</code></pre> <p>Set parameters for the model.</p> <p>This method updates the parameters of the model stored in the <code>self.model</code> attribute. It uses the provided dictionary of parameters to set new values for the model's parameters.</p> <p>Parameters:</p> <ul> <li> <code>params</code>           \u2013            <p>A dictionary containing the parameter names and values to be set for the model. The keys should be the names of the parameters, and the values should be the desired values for those parameters.</p> </li> </ul> <p>Examples: <pre><code>reg = Regession(dataset, algorithm='linear_regression')\nnew_params = {'alpha': 0.1, 'max_iter': 1000}\nreg.set_params(new_params)\nprint(reg.model[1].get_params())\n# output : {'alpha': 0.1, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': 'deprecated', ...}\n</code></pre></p>"},{"location":"classification/","title":"Classification","text":""},{"location":"classification/#optialgo.Classification","title":"Classification","text":"<pre><code>Classification(dataset: Dataset, algorithm: Optional[Literal['Naive Bayes', 'K-Nearest Neighbor', 'SVM', 'Logistic Regression', 'Random Forest', 'Decision Tree', 'XGBoost', 'Gradient Boosting']] = None)\n</code></pre> <p>               Bases: <code>Parent</code></p> <p>If the dataset represents a binary classification problem, it adds the AUC metric to the classification metrics.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>           \u2013            <p>An instance of the Dataset class, which includes the data and its characteristics.</p> </li> <li> <code>algorithm</code>           \u2013            <p>The algorithm to be used for classification. Default is None.</p> </li> </ul> <p>Examples: <pre><code>dataset = Dataset(dataframe, norm=True, test_size=0.3, seed=123)\ndataset.fit(features=features,target=target)\nclassifier = Classifier(dataset, algorithm='random_forest')\n</code></pre></p>"},{"location":"classification/#optialgo.Classification.compare_model","title":"compare_model","text":"<pre><code>compare_model(output: Literal['dict', 'dataframe', 'table', 'only_score'] = 'dict', train_val: bool = True, n_splits: int = 5, verbose: bool = True)\n</code></pre> <p>Compares multiple classification models based on various metrics and returns the results.</p> <p>This function runs a set of algorithms on the training data, evaluates them on the training and validation sets or using cross-validation, and compiles the performance metrics. The results can be output in different formats, including dictionary, pandas DataFrame, or a formatted table.</p> <p>Parameters:</p> <ul> <li> <code>output</code>           \u2013            <p>The format of the output. It can be: - \"dict\": Returns the results as a dictionary (default). - \"dataframe\": Returns the results as a pandas DataFrame. - \"table\": Prints the results as a formatted table. - \"only_score\": Returns only the accuracy scores in a simplified dictionary.</p> </li> </ul> If True, the function evaluates the models using a train-validation split. <p>If False, the function uses cross-validation. Default is True.</p> <p>Returns:</p> <ul> <li>           \u2013            <p>dict or pd.DataFrame or None</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the task type is not \"classification\" or \"regression\".</p> </li> </ul> <p>Examples: <pre><code>dataset = Dataset(dataframe, norm=True, test_size=0.3, seed=123)\ndataset.fit(features=features,target=target,t=\"classification\")\nclassifier = Classification(dataset, algorithm='random_forest')\nresults = classifier.compare_model(output='dataframe', train_val=True)\nprint(results)\n</code></pre></p>"},{"location":"classification/#optialgo.Classification.score","title":"score","text":"<pre><code>score(y_true, y_pred, metric='accuracy')\n</code></pre> <p>Calculates the score based on the predicted and true target values.</p> <p>Parameters:</p> <ul> <li> <code>y_true</code>               (<code>array - like</code>)           \u2013            <p>The true target values.</p> </li> <li> <code>y_pred</code>               (<code>array - like</code>)           \u2013            <p>The predicted target values.</p> </li> <li> <code>metric</code>               (<code>str</code>, default:                   <code>'accuracy'</code> )           \u2013            <p>The scoring metric to use. Default is 'accuracy'.                     Other options depend on the problem type:                     - For multiclass classification: 'precision', 'recall', 'f1', 'accuracy'                     - For binary classification or regression: 'accuracy', 'precision', 'recall', 'f1', 'r2'                     See sklearn.metrics for available metrics.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>          \u2013            <p>The score calculated based on the specified metric.</p> </li> </ul> Note <p>This method calculates the score based on the predicted and true target values. For multiclass classification, it uses weighted averaging for precision, recall, and F1 score. For other problem types, it returns the specified metric score directly.</p>"},{"location":"classification/#optialgo.Classification.score_report","title":"score_report","text":"<pre><code>score_report(y_true, y_pred)\n</code></pre> <p>Generates a report containing various metric scores and classification report between true and predicted target values.</p> <p>Parameters:</p> <ul> <li> <code>y_true</code>               (<code>array - like</code>)           \u2013            <p>The true target values.</p> </li> <li> <code>y_pred</code>               (<code>array - like</code>)           \u2013            <p>The predicted target values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>A dictionary containing metric scores calculated for each metric defined in the METRICS dictionary   along with the classification report.</p> </li> </ul> Note <p>This method generates a report containing various metric scores and classification report between true and predicted target values. It calculates metric scores for each metric defined in the METRICS dictionary and includes the classification report generated using scikit-learn's classification_report function.</p>"},{"location":"dataset/","title":"Dataset","text":""},{"location":"dataset/#optialgo.Dataset","title":"Dataset","text":"<pre><code>Dataset(dataframe: DataFrame, norm: bool = False, test_size: float = 0.2, seed: int = 42)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>dataframe</code>               (<code>DataFrame</code>)           \u2013            <p>The input dataframe containing the data to be processed and used for model training.</p> </li> <li> <code>norm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>A flag indicating whether to normalize the features. Default is False.</p> </li> <li> <code>test_size</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The proportion of the dataset to include in the test split. Default is 0.2 (20%).</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>The random seed for reproducibility of the train-test split. Default is 42.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If there are missing values in the input dataframe, an error is raised with information on which columns contain missing values.</p> </li> </ul> <p>Examples: <pre><code>df = pd.read_csv(\"house_prices.csv\")\ndataset = Dataset(dataframe=df)\n</code></pre></p>"},{"location":"dataset/#optialgo.Dataset.fit","title":"fit","text":"<pre><code>fit(features: list, target: Optional[str], t: Literal['classification', 'regression'], encoder: dict = None, ci=False)\n</code></pre> <p>Prepares and fits the dataset for a machine learning task by performing necessary preprocessing steps.</p> <p>Parameters:</p> <ul> <li> <code>features</code>               (<code>list</code>)           \u2013            <p>A list of feature column names to be used for model training.</p> </li> <li> <code>target</code>               (<code>Optional[str]</code>)           \u2013            <p>The name of the target column.</p> </li> <li> <code>t</code>               (<code>Literal['classification', 'regression']</code>)           \u2013            <p>classification or regression</p> </li> <li> <code>encoder</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary specifying custom encoders for specific columns. If None, default encoders are used.</p> </li> <li> <code>check_imbalance</code>           \u2013            <p>If True, checks for class imbalance in the target column for classification tasks. If an imbalance is detected, it triggers the imbalance handling procedure.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>t</code>           \u2013            <p>The type of machine learning task (<code>clustering</code>, <code>classification</code>, or <code>regression</code>).</p> </li> <li> <code>class_type</code>           \u2013            <p>The classification type (<code>binary</code> or <code>multiclass</code>) if the task is classification.</p> </li> <li> <code>train</code>           \u2013            <p>The preprocessed training dataset.</p> </li> <li> <code>test</code>           \u2013            <p>The preprocessed testing dataset.</p> </li> <li> <code>pipeline</code>           \u2013            <p>The preprocessing pipeline used for transforming the data.</p> </li> <li> <code>features</code>           \u2013            <p>The list of feature names after preprocessing.</p> </li> <li> <code>feature_names</code>           \u2013            <p>The original feature names before preprocessing.</p> </li> <li> <code>target</code>           \u2013            <p>The target column name.</p> </li> <li> <code>label_encoder</code>           \u2013            <p>The label encoder used for encoding the target column if it is categorical.</p> </li> </ul> Notes <p>This method performs the following steps:     <pre><code>1. Determines the task type (clustering, classification, or regression) based on the target column.\n2. Encodes the target column if it is categorical.\n3. Checks for class imbalance if `ci` is True.\n4. Splits the dataset into training and testing sets.\n5. Applies preprocessing to the features using the specified or default encoders.\n6. Stores the preprocessed training and testing datasets for model training.\n</code></pre></p> <p>Examples: <pre><code>dataset = Dataset(dataframe=df)\ndataset.fit(features=[\"feature1\", \"feature2\", \"feature_3\"], target='target_column', t= \"classification\", check_imbalance=True)\n\n# if with custom encoder\nencoder = {\"one_hot\":[\"feature_1\",\"feature_2\"], \"target_mean\": [\"feature_3\"]}\ndataset.fit(features=[\"feature1\", \"feature2\", \"feature_3\"], target='target_column', t= \"classification\", check_imbalance=True, encoder= encoder)\n</code></pre></p>"},{"location":"dataset/#optialgo.Dataset.flow_from_array","title":"flow_from_array","text":"<pre><code>flow_from_array(X: ndarray) -&gt; ndarray\n</code></pre> <p>Transforms the input array using the preprocessing pipeline.</p> <p>Parameters:</p> <ul> <li> <code>X</code>           \u2013            <p>The input array to be transformed. It should have the same number of features as the training data and in the same order.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The transformed data as a NumPy array.</p> </li> </ul> <p>Examples: <pre><code>input_array = np.array([[1, 2, 3], [4, 5, 6]])\ntransformed_array = dataset.flow_from_array(input_array)\nprint(transformed_array.shape)\n# output : (2, 3)\n# Assuming the pipeline transforms it into 3 features\n</code></pre></p>"},{"location":"dataset/#optialgo.Dataset.flow_from_dataframe","title":"flow_from_dataframe","text":"<pre><code>flow_from_dataframe(X: DataFrame) -&gt; ndarray\n</code></pre> <p>Transforms the input dataframe using the preprocessing pipeline.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>DataFrame</code>)           \u2013            <p>The input dataframe to be transformed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The transformed data as a NumPy array.</p> </li> </ul> <p>Examples: <pre><code>new_data = pd.DataFrame({\"col_a\": [1,2,3],\"col_b\":[1,1,1], \"col_c\" : [2,2,2]})\ntransformed_X = dataset.flow_from_dataframe(new_data)\nprint(transformed_X.shape)\n# output : (3,3)\n</code></pre></p>"},{"location":"dataset/#optialgo.Dataset.get_label","title":"get_label","text":"<pre><code>get_label(y_pred: ndarray) -&gt; ndarray\n</code></pre> <p>Converts predicted numerical labels back to their original categorical labels using the label encoder.</p> <p>Parameters:</p> <ul> <li> <code>y_pred</code>               (<code>ndarray</code>)           \u2013            <p>The predicted numerical labels to be converted.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The original categorical labels corresponding to the numerical predictions.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the label encoder has not been fitted or is not available.</p> </li> </ul> <p>Examples: <pre><code>y_pred = np.array([0, 1, 2])\noriginal_labels = dataset.get_label(y_pred)\nprint(original_labels)\n# output : array(['class1', 'class2', 'class3'], dtype=object)\n</code></pre></p>"},{"location":"dataset/#optialgo.Dataset.get_x_y","title":"get_x_y","text":"<pre><code>get_x_y()\n</code></pre> <p>Splits the preprocessed training and testing data into features and target arrays.</p> <p>Returns:</p> <ul> <li> <code>tuple</code>          \u2013            <p>A tuple containing four elements: X_train, X_test, y_train, y_test</p> </li> </ul> <p>Examples: <pre><code>X_train, X_test, y_train, y_test = dataset.get_x_y()\n</code></pre></p>"},{"location":"dataset/#optialgo.Dataset.save","title":"save","text":"<pre><code>save()\n</code></pre> <p>Serialize and save the dataset object using pickle.</p> <p>Returns:</p> <ul> <li> <code>bytes</code>          \u2013            <p>Serialized representation of the optialgo object.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>PickleError</code>             \u2013            <p>If serialization fails.</p> </li> </ul>"},{"location":"dataset/#properties","title":"Properties","text":""},{"location":"dataset/#optialgo.Dataset.ENCODERS","title":"ENCODERS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENCODERS = {'target_mean': TargetEncoder(), 'one_hot': OneHotEncoder(), 'ordinal': OrdinalEncoder()}\n</code></pre>"},{"location":"dataset/#optialgo.Dataset.dataframe","title":"dataframe  <code>property</code>","text":"<pre><code>dataframe\n</code></pre>"},{"location":"dataset/#optialgo.Dataset.train","title":"train  <code>property</code>","text":"<pre><code>train: DataFrame\n</code></pre>"},{"location":"dataset/#optialgo.Dataset.test","title":"test  <code>property</code>","text":"<pre><code>test\n</code></pre>"},{"location":"dataset/#optialgo.Dataset.feature_names","title":"feature_names  <code>property</code>","text":"<pre><code>feature_names\n</code></pre>"},{"location":"dataset/#optialgo.Dataset.label_encoder","title":"label_encoder  <code>property</code>","text":"<pre><code>label_encoder\n</code></pre>"},{"location":"dataset/#optialgo.Dataset.pipeline","title":"pipeline  <code>property</code>","text":"<pre><code>pipeline\n</code></pre>"},{"location":"dataset/#optialgo.Dataset.target","title":"target  <code>property</code>","text":"<pre><code>target\n</code></pre>"},{"location":"regression/","title":"Regression","text":""},{"location":"regression/#optialgo.Regression","title":"Regression","text":"<pre><code>Regression(dataset: Dataset, algorithm: str = None)\n</code></pre> <p>               Bases: <code>Parent</code></p>"},{"location":"regression/#optialgo.Regression.compare_model","title":"compare_model","text":"<pre><code>compare_model(output: str = 'dict', train_val: bool = False, n_splits: int = 5, verbose: bool = True)\n</code></pre> <p>Compares multiple regression models based on various metrics and returns the results.</p> <p>This function evaluates a set of algorithms on the training and validation sets or using cross-validation, and compiles performance metrics. The results can be output in different formats, including dictionary, pandas DataFrame, or a formatted table.</p> <p>Parameters:</p> <ul> <li> <code>output</code>           \u2013            <p>The format of the output. It can be: - \"dict\": Returns the results as a dictionary (default). - \"dataframe\": Returns the results as a pandas DataFrame. - \"table\": Prints the results as a formatted table. - \"only_score\": Returns only the Mean Absolute Percentage Error (MAPE) scores in a simplified dictionary.</p> </li> <li> <code>train_val</code>           \u2013            <p>If True, the function evaluates the models using a train-validation split. If False, the function uses cross-validation. Default is False.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>dict or pd.DataFrame or None: The function returns results based on the <code>output</code> parameter: - If <code>output</code> is \"dict\", it returns a dictionary of the results. - If <code>output</code> is \"dataframe\", it returns a pandas DataFrame of the results. - If <code>output</code> is \"table\", it prints the results as a formatted table. - If <code>output</code> is \"only_score\", it returns a dictionary with only the MAPE scores.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an invalid output type is provided.</p> </li> </ul> <p>Examples: <pre><code>dataset = Dataset(dataframe, norm=True, test_size=0.3, seed=123)\ndataset.fit(features=features,target=target)\nregressor = Regressor(dataset, algorithm='random_forest')\nresults = regressor.compare_model(output='dataframe', train_val=True)\nprint(results)\n</code></pre></p>"},{"location":"regression/#optialgo.Regression.score","title":"score","text":"<pre><code>score(y_true, y_pred, metric='mean_absolute_error')\n</code></pre> <p>Calculate the evaluation metric for the given true and predicted values.</p> <p>This method computes a specified error metric based on the true and predicted values of the target variable. It uses the metrics stored in the <code>self.METRICS</code> dictionary.</p> <p>Parameters:</p> <ul> <li> <code>y_true</code>           \u2013            <p>True values of the target variable.</p> </li> <li> <code>y_pred</code>           \u2013            <p>Predicted values of the target variable.</p> </li> <li> <code>metric</code>           \u2013            <p>The metric to be used for evaluating the predictions. Default is \"mean_absolute_error\". Possible values include: - \"mean_absolute_error\" - \"mean_squared_error\" - \"mean_absolute_percentage_error\" - Other metrics defined in <code>self.METRICS</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>          \u2013            <p>The computed error metric value.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If the specified metric is not found in <code>self.METRICS</code>.</p> </li> </ul> <p>Examples: <pre><code>y_true = [3.0, -0.5, 2.0, 7.0]\ny_pred = [2.5, 0.0, 2.0, 8.0]\nregressor = Regressor(dataset, algorithm='linear_regression')\nmae = regressor.score(y_true, y_pred, metric='mean_absolute_error')\nprint(mae)\n# output : 0.5\n</code></pre></p>"},{"location":"regression/#optialgo.Regression.score_report","title":"score_report","text":"<pre><code>score_report(y_true, y_pred)\n</code></pre> <p>Generate a report of evaluation metrics for the given true and predicted values.</p> <p>This method computes various evaluation metrics defined in <code>self.METRICS</code> for the provided true and predicted values of the target variable. It returns a dictionary with the names of the metrics as keys and their computed values as values.</p> <p>Parameters:</p> <ul> <li> <code>y_true</code>           \u2013            <p>True values of the target variable.</p> </li> <li> <code>y_pred</code>           \u2013            <p>Predicted values of the target variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>A dictionary containing the names and values of the computed metrics. The keys are</p> </li> <li>           \u2013            <p>the metric names (as defined in <code>self.METRICS</code>) and the values are the corresponding</p> </li> <li>           \u2013            <p>metric values.</p> </li> </ul> <p>Examples: <pre><code>y_true = [3.0, -0.5, 2.0, 7.0]\ny_pred = [2.5, 0.0, 2.0, 8.0]\nregressor = Regressor(dataset, algorithm='linear_regression')\nreport = regressor.score_report(y_true, y_pred)\nfor metric, value in report.items():\n    print(f\"{metric}: {value:.4f}\")\n''' output :\nmean_absolute_error: 0.5000\nmean_squared_error: 0.3750\nmean_absolute_percentage_error: 0.1271\n'''\n</code></pre></p>"},{"location":"text_dataset/","title":"Text dataset","text":""},{"location":"text_dataset/#optialgo.TextDataset","title":"TextDataset","text":"<pre><code>TextDataset(dataframe: DataFrame, test_size: float = 0.2, seed: int = 42)\n</code></pre> <p>A class to handle text data preprocessing and manipulation for machine learning tasks.</p> <p>Parameters:</p> <ul> <li> <code>dataframe</code>               (<code>DataFrame</code>)           \u2013            <p>The input dataframe containing the dataset.</p> </li> <li> <code>test_size</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>Proportion of the dataset to include in the test split (default is 0.2).</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for reproducibility (default is 42).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If there are missing values in the dataframe.</p> </li> </ul>"},{"location":"text_dataset/#optialgo.TextDataset.fit","title":"fit","text":"<pre><code>fit(feature: str, target: Optional[str], t: Literal['classification', 'regression'], vectorizer: Union[Literal['tfidf', 'count_vect'], Tokenizer] = 'tfidf', ci: bool = False)\n</code></pre> <p>Fits the preprocessing pipeline and prepares the data for training and testing.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>str</code>)           \u2013            <p>The feature column to use for training.</p> </li> <li> <code>target</code>           \u2013            <p>The target column to use for training.</p> </li> <li> <code>t</code>           \u2013            <p>The type of task (classification or regression).</p> </li> <li> <code>vectorizer</code>           \u2013            <p>The vectorizer to use for preprocessing (default is \"tfidf\").</p> </li> <li> <code>verbose</code>           \u2013            <p>Whether to print verbose output (default is False).</p> </li> <li> <code>ci</code>           \u2013            <p>Whether to check for class imbalance (default is False).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TextDataset</code>          \u2013            <p>The fitted TextDataset object.</p> </li> </ul>"},{"location":"text_dataset/#optialgo.TextDataset.flow_from_array","title":"flow_from_array","text":"<pre><code>flow_from_array(X: ndarray) -&gt; ndarray\n</code></pre> <p>Transforms the given array using the preprocessing pipeline.</p> <p>Parameters:</p> <ul> <li> <code>X</code>           \u2013            <p>The input array to transform.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The transformed array.</p> </li> </ul>"},{"location":"text_dataset/#optialgo.TextDataset.flow_from_dataframe","title":"flow_from_dataframe","text":"<pre><code>flow_from_dataframe(X: DataFrame) -&gt; ndarray\n</code></pre> <p>Transforms the feature column of the given dataframe using the preprocessing pipeline.</p> <p>Parameters:</p> <ul> <li> <code>X</code>           \u2013            <p>The input dataframe to transform.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The transformed feature column.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If the feature column is not in the dataframe.</p> </li> </ul>"},{"location":"text_dataset/#optialgo.TextDataset.get_label","title":"get_label","text":"<pre><code>get_label(y_pred: ndarray) -&gt; ndarray\n</code></pre> <p>Converts predicted labels back to their original form using the label encoder.</p> <p>Parameters:</p> <ul> <li> <code>y_pred</code>           \u2013            <p>The predicted labels.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The original labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the label encoder is not found.</p> </li> </ul>"},{"location":"text_dataset/#optialgo.TextDataset.get_x_y","title":"get_x_y","text":"<pre><code>get_x_y() -&gt; tuple\n</code></pre> <p>Returns the train and test splits of features and labels.</p> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>The training features, testing features, training labels, and testing labels.</p> </li> </ul>"},{"location":"text_dataset/#properties","title":"Properties","text":""},{"location":"text_dataset/#optialgo.TextDataset.dataframe","title":"dataframe  <code>property</code>","text":"<pre><code>dataframe\n</code></pre>"},{"location":"text_dataset/#optialgo.TextDataset.train","title":"train  <code>property</code>","text":"<pre><code>train\n</code></pre>"},{"location":"text_dataset/#optialgo.TextDataset.test","title":"test  <code>property</code>","text":"<pre><code>test\n</code></pre>"},{"location":"text_dataset/#optialgo.TextDataset.pipeline","title":"pipeline  <code>property</code>","text":"<pre><code>pipeline\n</code></pre>"},{"location":"text_dataset/#optialgo.TextDataset.feature","title":"feature  <code>property</code>","text":"<pre><code>feature\n</code></pre>"},{"location":"text_dataset/#optialgo.TextDataset.vectorizer","title":"vectorizer  <code>property</code>","text":"<pre><code>vectorizer\n</code></pre>"},{"location":"text_dataset/#optialgo.TextDataset.label_encoder","title":"label_encoder  <code>property</code>","text":"<pre><code>label_encoder\n</code></pre>"},{"location":"tutorials/make_dataset/","title":"Make Dataset","text":"<p>the assumption is that <code>Dataset</code> is like pandas dataframe. you don't need preprocessing like doing <code>ColumnTransformer</code>, everything is already done in <code>Dataset</code> \ud83e\udee1.  <pre><code>from optialgo import Dataset,TextDataset\nimport pandas as pd\n\ndf = pd.read_csv(\"dataset_ex/drug200.csv\")\n\ndataset = Dataset(df,test_size=0.2) # if you want the data to be normalized then set `norm=True`\nprint(dataset.dataframe)\n</code></pre> <pre><code>     Age Sex      BP Cholesterol  Na_to_K   Drug\n0     23   F    HIGH        HIGH   25.355  DrugY\n1     47   M     LOW        HIGH   13.093  drugC\n2     47   M     LOW        HIGH   10.114  drugC\n3     28   F  NORMAL        HIGH    7.798  drugX\n4     61   F     LOW        HIGH   18.043  DrugY\n..   ...  ..     ...         ...      ...    ...\n195   56   F     LOW        HIGH   11.567  drugC\n196   16   M     LOW        HIGH   12.006  drugC\n197   52   M  NORMAL        HIGH    9.894  drugX\n198   23   M  NORMAL      NORMAL   14.020  drugX\n199   40   F     LOW      NORMAL   11.349  drugX\n\n[200 rows x 6 columns]\n</code></pre> Fit features and target :</p> <pre><code>features = df.columns[:-1] # ['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']\ntarget = \"Drug\" # column target\n\ndataset.fit(features=features,target=target,t=\"classification\") \nprint(dataset)\n</code></pre> <pre><code>&lt;Dataset classification Object&gt;\n</code></pre> <p>after fit, you get train set and test set.</p> <pre><code>print(dataset.train)\n</code></pre> <pre><code>          Sex        BP  Cholesterol   Age  Na_to_K  Drug\n0    1.459026  1.885476     1.670576  62.0   27.183     0\n1    1.459026  2.413025     1.513065  53.0   14.133     4\n2    1.746044  1.885476     1.670576  65.0   13.769     4\n3    1.746044  1.885476     1.670576  51.0   23.003     0\n4    1.459026  2.413025     1.670576  35.0    7.845     4\n..        ...       ...          ...   ...      ...   ...\n155  1.746044  2.413025     1.670576  18.0    8.750     4\n156  1.459026  0.710399     1.670576  60.0    8.621     2\n157  1.459026  1.885476     1.513065  16.0   12.006     3\n158  1.746044  2.413025     1.513065  28.0   12.879     4\n159  1.746044  1.885476     1.670576  40.0   11.349     4\n\n[160 rows x 6 columns]\n</code></pre> <pre><code>print(dataset.test)\n</code></pre> <pre><code>         Sex        BP  Cholesterol   Age  Na_to_K  Drug\n0   1.746044  2.413025     1.513065  49.0   16.275     0\n1   1.746044  0.710399     1.670576  32.0   10.292     1\n2   1.746044  0.710399     1.513065  73.0   18.348     0\n3   1.459026  0.710399     1.670576  22.0   28.294     0\n4   1.746044  2.413025     1.670576  66.0    8.107     4\n5   1.459026  2.413025     1.513065  23.0   12.260     4\n6   1.459026  1.885476     1.670576  49.0   11.014     4\n7   1.459026  2.413025     1.513065  64.0    7.761     4\n8   1.459026  1.885476     1.513065  23.0    7.298     3\n9   1.459026  1.885476     1.513065  47.0   13.093     3\n10  1.746044  1.885476     1.670576  38.0   29.875     0\n11  1.459026  0.710399     1.513065  43.0   13.972     1\n12  1.746044  0.710399     1.513065  18.0   37.188     0\n13  1.459026  2.413025     1.670576  67.0    9.514     4\n14  1.746044  0.710399     1.670576  38.0   11.326     1\n15  1.746044  1.885476     1.513065  61.0   18.043     0\n16  1.746044  1.885476     1.513065  28.0   19.796     0\n17  1.746044  1.885476     1.513065  33.0   33.486     0\n18  1.746044  2.413025     1.513065  51.0   13.597     4\n19  1.746044  2.413025     1.513065  70.0   20.489     0\n20  1.746044  0.710399     1.670576  19.0   25.969     0\n21  1.746044  0.710399     1.513065  20.0   11.262     1\n22  1.746044  1.885476     1.513065  58.0   26.645     0\n23  1.746044  2.413025     1.513065  30.0   10.443     4\n24  1.746044  0.710399     1.670576  21.0   28.632     0\n25  1.746044  2.413025     1.513065  73.0   19.221     0\n26  1.459026  2.413025     1.670576  50.0   15.790     0\n27  1.746044  2.413025     1.513065  36.0   16.753     0\n28  1.459026  0.710399     1.513065  50.0    7.490     1\n29  1.459026  1.885476     1.670576  49.0   13.598     4\n30  1.459026  1.885476     1.670576  26.0   20.909     0\n31  1.746044  1.885476     1.670576  72.0   14.642     4\n32  1.459026  1.885476     1.513065  42.0   20.013     0\n33  1.746044  0.710399     1.670576  22.0   22.818     0\n34  1.459026  0.710399     1.513065  68.0   11.009     2\n35  1.459026  0.710399     1.513065  70.0    9.849     2\n36  1.459026  1.885476     1.670576  35.0    9.170     4\n37  1.459026  0.710399     1.513065  60.0   13.934     2\n38  1.459026  1.885476     1.513065  41.0   11.037     3\n39  1.459026  2.413025     1.513065  61.0    9.443     4\n</code></pre>"},{"location":"tutorials/make_dataset/#customization-columntransform","title":"Customization <code>ColumnTransform</code>","text":"<ul> <li><code>target_mean</code> : (https://hackernoon.com/the-concept-behind-mean-target-encoding-in-ai-and-ml)</li> <li><code>one_hot</code> : (https://medium.com/@WojtekFulmyk/one-hot-encoding-a-brief-explanation-8c5daec395e3)</li> <li><code>ordinal</code> : (https://medium.com/@WojtekFulmyk/ordinal-encoding-a-brief-explanation-a29cf374dbc1) <pre><code>cat_encoders = {\"ordinal\":[\"BP\",\"Cholesterol\"],\"one_hot\":[\"Sex\"]}\ndataset.fit(features=features,target=target,t=\"classification\",encoder=cat_encoders)\nprint(dataset.train)\n</code></pre> <pre><code>      BP  Cholesterol  Sex_F  Sex_M   Age  Na_to_K  Drug\n0    1.0          1.0    0.0    1.0  62.0   27.183     0\n1    2.0          2.0    0.0    1.0  53.0   14.133     4\n2    1.0          1.0    1.0    0.0  65.0   13.769     4\n3    1.0          1.0    1.0    0.0  51.0   23.003     0\n4    2.0          1.0    0.0    1.0  35.0    7.845     4\n..   ...          ...    ...    ...   ...      ...   ...\n155  2.0          1.0    1.0    0.0  18.0    8.750     4\n156  3.0          1.0    0.0    1.0  60.0    8.621     2\n157  1.0          2.0    0.0    1.0  16.0   12.006     3\n158  2.0          2.0    1.0    0.0  28.0   12.879     4\n159  1.0          1.0    1.0    0.0  40.0   11.349     4\n\n[160 rows x 7 columns]\n</code></pre></li> </ul>"},{"location":"tutorials/make_dataset/#text-dataset","title":"Text Dataset","text":"<pre><code>import pandas as pd\nfrom optialgo import TextDataset\n\ndf_text = pd.read_csv(\"dataset_ex/IMDB Dataset.csv\")[:10] # for example\ndataset = TextDataset(df_text,test_size=0.1)\n\nfeature = \"review\" # the text field that will be used as a feature\ntarget = \"sentiment\"\n\ndataset.fit(feature=feature,target=target,t=\"classification\",vectorizer='tfidf')\nprint(dataset.train)\n</code></pre> <pre><code>[[0.         0.         0.         ... 0.         0.         1.        ]\n [0.         0.10308732 0.         ... 0.         0.         1.        ]\n [0.         0.         0.         ... 0.04835567 0.         1.        ]\n ...\n [0.         0.         0.         ... 0.         0.         1.        ]\n [0.07186978 0.         0.         ... 0.         0.17018334 0.        ]\n [0.06504008 0.         0.         ... 0.         0.         1.        ]]\n</code></pre>"},{"location":"tutorials/model/","title":"Model","text":"<p>You can perform classification very easily by simply entering a pre-made <code>dataset</code>\ud83e\udee8. </p> <pre><code>from optialgo import Classification\n\nclf = Classification(dataset=dataset)\nprint(clf)\n</code></pre> <pre><code>&lt;Classificaton Object&gt;\n</code></pre>"},{"location":"tutorials/model/#compare-models","title":"Compare Models","text":"<p>Notes : <pre><code>output : \n- table : only displays the output table\n- dataframe : returns the dataframe \n- only_accuracy : only returns the accuracy score\n\ntrain_val : using train test split or using cross validation \nverbose : displaying progress or not\n</code></pre></p> <pre><code>print(clf.compare_model(output=\"dataframe\",train_val=True,verbose=True))\n</code></pre> <pre><code>                     accuracy_train  accuracy_val  precision_train  precision_val  recall_train  recall_val  f1_train    f1_val\nNaive Bayes                 0.58750         0.500         0.595715       0.586250       0.58750       0.500  0.539619  0.496820\nK-Nearest Neighbor          0.79375         0.675         0.802155       0.660000       0.79375       0.675  0.790456  0.660161\nSVM                         0.70000         0.700         0.547344       0.530451       0.70000       0.700  0.605803  0.598718\nLogistic Regression         0.98125         0.975         0.981663       0.976316       0.98125       0.975  0.981207  0.974743\nRandom Forest               1.00000         1.000         1.000000       1.000000       1.00000       1.000  1.000000  1.000000\nDecision Tree               1.00000         0.975         1.000000       0.976316       1.00000       0.975  1.000000  0.974743\nXGBoost                     1.00000         0.975         1.000000       0.981250       1.00000       0.975  1.000000  0.976190\nGradient Boosting           1.00000         0.950         1.000000       0.951316       1.00000       0.950  1.000000  0.948949\n</code></pre>"},{"location":"tutorials/model/#set-algorithm","title":"Set Algorithm","text":"<p>Having found the <code>compare_model</code> score, you can now determine what algorithm to use \ud83d\ude09. <pre><code>actually, you can specify the algorithm to be used at the beginning.\nclf = Classification(dataset=dataset,algorithm=\"Logistic Regression\")\n</code></pre> <pre><code>clf.set_model(\"Logistic Regression\")\n</code></pre></p>"},{"location":"tutorials/model/#find-best-hyperparameters","title":"Find Best HyperParameters","text":"<p><pre><code>print(clf.get_params_from_model)\n</code></pre> <pre><code>{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 4000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n</code></pre> for an example of this case, see LogisticRegression</p> <p>you can change the parameter above, with the following estimation :</p> <pre><code>params = {\"C\":[1.0,1.2,1.5,2.0],\"solver\":[\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\nbest_score,best_params = clf.find_best_params(params)\nprint(f\"best_score : {best_score} | best_params : {best_params}\")\n</code></pre> <pre><code>best_score : 0.9625 | best_params : {'C': 2.0, 'solver': 'lbfgs'}\n</code></pre>"},{"location":"tutorials/model/#tuning-hyperparameter","title":"Tuning HyperParameter","text":"<p>after finding the best hyperparameter, you can tune the hyperparameter. <pre><code>clf.set_params(best_params)\n</code></pre></p>"},{"location":"tutorials/model/#prediction","title":"Prediction","text":"<p>After determining the algorithm, you can make predictions for the actual test data \ud83e\udd2b. <pre><code>new_data = [[22,\"F\",\"HIGH\",\"HIGH\",23.1]]\nprint(clf.predict(new_data))\n</code></pre> <pre><code>['DrugY']\n</code></pre></p> <p>Note: For the regression case, you can simply call the <code>Regression</code> class, but first make sure <code>t</code> in your dataset = <code>regression</code>.</p> <pre><code>import pandas as pd\nfrom optialgo import Dataset,Regression\n\ndf = pd.read_csv(\"dataset_ex/Housing.csv\") # Regression Dataset\ndataset = Dataset(df)\nfeatures = df.columns[1:] # ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'furnishingstatus']\ntarget = \"price\"\ndataset.fit(features=features,target=target,t=\"regression\")\n\nreg = Regression(dataset)\nprint(reg)\n</code></pre> <pre><code>&lt;Regression Object&gt;\n</code></pre>"},{"location":"tutorials/text_classification/","title":"Text Classification","text":"<pre><code>import pandas as pd\nfrom optialgo import text_clean,text_manipulation,TextDataset,Classification\n\ndf = pd.read_csv(\"dataset_ex/IMDB Dataset.csv\")[:200]\ntrain = df.sample(190)\ntest = df.sample(10)\n\nprint(train)\n</code></pre> <pre><code>                                                review sentiment\n127  The complaints are valid, to me the biggest pr...  negative\n44   This movie struck home for me. Being 29, I rem...  positive\n39   After sitting through this pile of dung, my hu...  negative\n93   If anyone is wondering why no one makes movies...  positive\n140  Before I begin, let me get something off my ch...  negative\n..                                                 ...       ...\n125  I am so happy and surprised that there is so m...  negative\n126  \"Revolt of the Zombies\" proves that having the...  negative\n174  I think this movie was supposed to be shocking...  negative\n133  This movie shows a clip of live animal mutilat...  negative\n147  Francis Ford Coppola wrote and directed this s...  positive\n\n[190 rows x 2 columns]\n</code></pre>"},{"location":"tutorials/text_classification/#preprocessing","title":"Preprocessing","text":"<pre><code>texts  = train['review'].tolist()\n\n# Text clean (remove : punctuation,digits,emoji,chars-non-latin,and urls)\ntexts = text_clean(texts,return_token=True)\n\n# Text manipulation \nadditional_st = ['s','b','br']\ntexts = text_manipulation(texts,lang=\"english\",stopwords=True,additional=additional_st)\ntrain['review'] = texts \nprint(df)\n</code></pre> <pre><code>                                                review sentiment\n0    One of the other reviewers has mentioned that ...  positive\n1    A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...  positive\n2    I thought this was a wonderful way to spend ti...  positive\n3    Basically there's a family where a little boy ...  negative\n4    Petter Mattei's \"Love in the Time of Money\" is...  positive\n..                                                 ...       ...\n195  Phantasm ....Class. Phantasm II.....awesome. P...  negative\n196  Ludicrous. Angelic 9-year-old Annakin turns in...  negative\n197  Scotty (Grant Cramer, who would go on to star ...  negative\n198  If you keep rigid historical perspective out o...  positive\n199  The film quickly gets to a major chase scene w...  negative\n\n[200 rows x 2 columns]\n</code></pre>"},{"location":"tutorials/text_classification/#make-dataset","title":"Make Dataset","text":"<pre><code>feature = \"review\"\ntarget = \"sentiment\"\n\ndataset = TextDataset(train)\ndataset.fit(feature=feature, target=target, t=\"classification\", vectorizer=\"tfidf\")\nprint(dataset.train)\n</code></pre> <pre><code>[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 1.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 1.]]\n</code></pre>"},{"location":"tutorials/text_classification/#model","title":"Model","text":"<pre><code>clf = Classification(dataset)\nprint(clf)\n</code></pre> <pre><code>&lt;Classificaton Object&gt;\n</code></pre>"},{"location":"tutorials/text_classification/#predict-sentiment","title":"Predict Sentiment","text":"<pre><code>clf.set_model(\"Random Forest\")\nprint(clf.predict(test[feature]))\n</code></pre> <pre><code>['negative' 'negative' 'positive' 'negative' 'positive' 'positive'\n 'negative' 'negative' 'negative' 'positive']\n</code></pre>"},{"location":"tutorials/text_preprocessing/","title":"Text Preprocessing","text":"<p>The <code>text_preprocessing</code> module can help you perform text cleaning &amp; manipulation quickly and efficiently \ud83e\udee0.</p>"},{"location":"tutorials/text_preprocessing/#text-cleaning","title":"Text Cleaning","text":"<p>You can clean text easily, just use the <code>text_clean</code> function.</p> <p>Note : for the arguments in function <code>text_clean</code>, see text_clean <pre><code>from optialgo.text_preprocessing import text_clean\ndataset = [\"I told you not to feed that dog!\",\"There are 25 students in my math class this semester.\",\n\"For more information about the project, please visit https://github.com/nsandarma/OptiAlgo\",\"My favorite Japanese word is '\u3042\u308a\u304c\u3068\u3046' which means 'thank you'.\",\"I'm so excited for the weekend! \ud83d\ude04\"\n]\ndataset_clean = text_clean(data=dataset) # if returns is tokens, set parameters `return_token` is `True`\nprint(dataset_clean)\n</code></pre> <pre><code>['i told you not to feed that dog ', 'there are  students in my math class this semester ', 'for more information about the project  please visit ', 'my favorite japanese word is    which means  thank you  ', 'i m so excited for the weekend  ']\n</code></pre></p>"},{"location":"tutorials/text_preprocessing/#remove-punctuation","title":"Remove Punctuation","text":"<p><pre><code>from optialgo.text_preprocessing import *\ndata = [\"I told you not to feed that dog!\"]\nprint(remove_punctuation(data))\n</code></pre> <pre><code>['I told you not to feed that dog ']\n</code></pre> Note:  use <code>f_remove_punctuation</code> if the data is <code>str</code> / single data</p>"},{"location":"tutorials/text_preprocessing/#remove-digits","title":"Remove Digits","text":"<p><pre><code>data = [\"There are 25 students in my math class this semester.\"]\nprint(remove_digits(data))\n</code></pre> <pre><code>['There are  students in my math class this semester.']\n</code></pre> Note:  use <code>f_remove_digits</code> if the data is <code>str</code> / single data</p>"},{"location":"tutorials/text_preprocessing/#remove-url","title":"Remove URL","text":"<p><pre><code>data = [\"For more information about the project, please visit https://github.com/nsandarma/OptiAlgo\"]\nprint(remove_url(data))\n</code></pre> <pre><code>['For more information about the project, please visit ']\n</code></pre> Note:  use <code>f_remove_url</code> if the data is <code>str</code> / single data</p>"},{"location":"tutorials/text_preprocessing/#remove-chars-non-latin","title":"Remove chars-non-latin","text":"<p><pre><code>data = [\"My favorite Japanese word is '\u3042\u308a\u304c\u3068\u3046' which means 'thank you'.\"]\nprint(remove_non_latin(data))\n</code></pre> <pre><code>[\"My favorite Japanese word is '' which means 'thank you'.\"]\n</code></pre> Note:  use <code>f_remove_non_latin</code> if the data is <code>str</code> / single data</p>"},{"location":"tutorials/text_preprocessing/#remove-emoji","title":"Remove Emoji","text":"<p><pre><code>data = [\"I'm so excited for the weekend! \ud83d\ude04\"]\nprint(remove_emoji(data))\n</code></pre> <pre><code>[\"I'm so excited for the weekend! \"]\n</code></pre> Note:  use <code>f_remove_emoji</code> if the data is <code>str</code> / single data</p>"},{"location":"tutorials/text_preprocessing/#text-manipulation","title":"Text Manipulation","text":"<p>After cleaning the text, you can manipulate the text by using the <code>text_manipulation</code> function \ud83e\udd76 .</p> <p>see : text_manipulation <pre><code># before performing `text_manipulation` the data must be tokenized first\nprint(text_manipulation(tokens=word_tokenize(dataset_clean),lang=\"english\")) # lang : [\"indonesian\",\"english\"]\n</code></pre> <pre><code>['i tell you not to fee that dog', 'there be student in my math class this semester', 'for more information about the project please visit', 'my favorite japanese word be which mean thank you', 'i m so excite for the weekend']\n</code></pre></p>"},{"location":"tutorials/text_preprocessing/#word-tokenize","title":"Word Tokenize","text":"<p><pre><code>tokens = word_tokenize(dataset_clean)\nprint(tokens)\n</code></pre> <pre><code>[('i', 'told', 'you', 'not', 'to', 'feed', 'that', 'dog'), ('there', 'are', 'students', 'in', 'my', 'math', 'class', 'this', 'semester'), ('for', 'more', 'information', 'about', 'the', 'project', 'please', 'visit'), ('my', 'favorite', 'japanese', 'word', 'is', 'which', 'means', 'thank', 'you'), ('i', 'm', 'so', 'excited', 'for', 'the', 'weekend')]\n</code></pre> Note:  use <code>f_word_tokenize</code> if the data is <code>str</code> / single data or <code>f_regex_word_tokenize</code> with regex.</p>"},{"location":"tutorials/text_preprocessing/#token-to-string","title":"Token To String","text":"<pre><code>print(token_to_str(tokens))\n</code></pre> <pre><code>['i told you not to feed that dog', 'there are students in my math class this semester', 'for more information about the project please visit', 'my favorite japanese word is which means thank you', 'i m so excited for the weekend']\n</code></pre>"},{"location":"tutorials/text_preprocessing/#word-normalization","title":"Word Normalization","text":"<p><pre><code>norm_words = {\"dog\":\"cat\"}\nprint(normalize(tokens,norm_words))\n</code></pre> <pre><code>[('i', 'told', 'you', 'not', 'to', 'feed', 'that', 'cat'), ('there', 'are', 'students', 'in', 'my', 'math', 'class', 'this', 'semester'), ('for', 'more', 'information', 'about', 'the', 'project', 'please', 'visit'), ('my', 'favorite', 'japanese', 'word', 'is', 'which', 'means', 'thank', 'you'), ('i', 'm', 'so', 'excited', 'for', 'the', 'weekend')]\n</code></pre> Note:  use <code>f_word_normalize</code> if the data is <code>str</code> / single data</p>"},{"location":"tutorials/text_preprocessing/#stopwords-removal","title":"Stopwords Removal","text":"<p><pre><code>print(remove_stopwords(tokens,lang=\"english\"))\n</code></pre> <pre><code>[('told', 'feed', 'dog'), ('students', 'math', 'class', 'semester'), ('information', 'project', 'please', 'visit'), ('favorite', 'japanese', 'word', 'means', 'thank'), ('excited', 'weekend')]\n</code></pre> You can also add additional stopwords :</p> <pre><code>additional_st = [\"dog\"]\nprint(remove_stopwords(tokens,lang=\"english\",additional=additional_st))\n</code></pre> <pre><code>[('told', 'feed'), ('students', 'math', 'class', 'semester'), ('information', 'project', 'please', 'visit'), ('favorite', 'japanese', 'word', 'means', 'thank'), ('excited', 'weekend')]\n</code></pre>"},{"location":"tutorials/text_preprocessing/#lemmatization","title":"Lemmatization","text":"<pre><code>print(lemmatization(tokens,lang=\"english\"))\n</code></pre> <pre><code>['i tell you not to fee that dog', 'there be student in my math class this semester', 'for more information about the project please visit', 'my favorite japanese word be which mean thank you', 'i m so excite for the weekend']\n</code></pre> <p>Note:  use <code>f_lemmatization_en</code> if the data is <code>str</code> / single data, <code>f_lemmatization_idn</code> for Indonesian text</p>"},{"location":"tutorials/text_preprocessing/#stemming","title":"Stemming","text":"<pre><code>print(stemming(tokens,lang=\"english\"))\n</code></pre> <pre><code>['i told you not to feed that dog', 'there are student in my math class thi semest', 'for more inform about the project pleas visit', 'my favorit japanes word is which mean thank you', 'i m so excit for the weekend']\n</code></pre> <p>Note:  use <code>f_stemming_en</code> if the data is <code>str</code> / single data, <code>f_stemming_idn</code> for Indonesian text</p>"},{"location":"tutorials/text_preprocessing/#tokenizer","title":"Tokenizer","text":"<p>If you need a tokenizer to prepare for deep learning, optialgo already provides it \ud83e\udef5</p> <p>see : tokenizer <pre><code>tokenizer = Tokenizer()\ntokenizer.fit(dataset_clean)\nprint(tokenizer.word_index)\n</code></pre> <pre><code>{'i': 1, 'my': 2, 'for': 3, 'the': 4, 'you': 5, 'so': 6, 'm': 7, 'thank': 8, 'means': 9, 'which': 10, 'about': 11, 'word': 12, 'japanese': 13, 'excited': 14, 'favorite': 15, 'visit': 16, 'please': 17, 'project': 18, 'is': 19, 'more': 20, 'information': 21, 'told': 22, 'semester': 23, 'this': 24, 'class': 25, 'math': 26, 'in': 27, 'students': 28, 'are': 29, 'there': 30, 'dog': 31, 'that': 32, 'feed': 33, 'to': 34, 'not': 35, 'weekend': 36}\n</code></pre></p>"},{"location":"tutorials/text_preprocessing/#count-words","title":"Count Words","text":"<pre><code>print(count_words(dataset_clean))\n</code></pre> <pre><code>Counter({'i': 2, 'you': 2, 'my': 2, 'for': 2, 'the': 2, 'told': 1, 'not': 1, 'to': 1, 'feed': 1, 'that': 1, 'dog': 1, 'there': 1, 'are': 1, 'students': 1, 'in': 1, 'math': 1, 'class': 1, 'this': 1, 'semester': 1, 'more': 1, 'information': 1, 'about': 1, 'project': 1, 'please': 1, 'visit': 1, 'favorite': 1, 'japanese': 1, 'word': 1, 'is': 1, 'which': 1, 'means': 1, 'thank': 1, 'm': 1, 'so': 1, 'excited': 1, 'weekend': 1})\n</code></pre>"},{"location":"tutorials/text_preprocessing/#texts-to-sequences","title":"Texts to Sequences","text":"<pre><code>sequences = tokenizer.texts_to_sequences(dataset_clean)\nprint(sequences)\n</code></pre> <pre><code>[[1, 22, 5, 35, 34, 33, 32, 31], [30, 29, 28, 27, 2, 26, 25, 24, 23], [3, 20, 21, 11, 4, 18, 17, 16], [2, 15, 13, 12, 19, 10, 9, 8, 5], [1, 7, 6, 14, 3, 4, 36]]\n</code></pre>"},{"location":"tutorials/text_preprocessing/#texts-to-pad-sequences","title":"Texts to Pad Sequences","text":"<pre><code>sequences = tokenizer.texts_to_pad_sequences(dataset_clean)\nprint(sequences)\n</code></pre> <pre><code>[[ 0  1 22  5 35 34 33 32 31]\n [30 29 28 27  2 26 25 24 23]\n [ 0  3 20 21 11  4 18 17 16]\n [ 2 15 13 12 19 10  9  8  5]\n [ 0  0  1  7  6 14  3  4 36]]\n</code></pre>"},{"location":"tutorials/text_preprocessing/#sequencespad-to-texts","title":"Sequences/Pad to Texts","text":"<pre><code>print(tokenizer.sequences_to_texts(sequences))\n</code></pre> <pre><code>['i told you not to feed that dog', 'there are students in my math class this semester', 'for more information about the project please visit', 'my favorite japanese word is which means thank you', 'i m so excited for the weekend']\n</code></pre>"},{"location":"utils/dimensionality_reduction/","title":"Dimensionality Reduction","text":""},{"location":"utils/dimensionality_reduction/#optialgo.pca","title":"pca","text":"<pre><code>pca(dataframe: DataFrame, features: list, n_components: int) -&gt; ndarray\n</code></pre> <p>Applies Principal Component Analysis (PCA) to reduce the dimensionality of the input data.</p> <p>Parameters:</p> <ul> <li> <code>dataframe</code>           \u2013            <p>The input dataframe containing the features.</p> </li> <li> <code>features</code>           \u2013            <p>A list of feature column names to be used for PCA.</p> </li> <li> <code>n_components</code>           \u2013            <p>The number of principal components to retain.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The transformed data after PCA.</p> </li> </ul> <p>Examples <pre><code>import pandas as pd\nfrom optialgo import PCA\ndf = pd.DataFrame({\n    'feature1': [1, 2, 3],\n    'feature2': [4, 5, 6],\n    'feature3': [7, 8, 9]\n})\ntransformed_data = pca(df, features=['feature1', 'feature2', 'feature3'], n_components=2)\ntransformed_data.shape\n# output : (3, 2)  # Assuming 3 samples and 2 principal components\n</code></pre></p>"},{"location":"utils/feature_selection/","title":"Feature Selection","text":""},{"location":"utils/feature_selection/#optialgo.feature_selection","title":"feature_selection","text":"<pre><code>feature_selection(dataframe: DataFrame, target: str, n_features: int, threshold: int = 0.0, features: list = None, show_score: bool = True) -&gt; dict\n</code></pre> <p>Perform feature selection using various methods and evaluate model performance.</p> <p>This function applies multiple feature selection methods to a given dataframe, fits a classification or regression model based on the target variable type, and evaluates the model performance for each set of selected features.</p> <p>Parameters:</p> <ul> <li> <code>dataframe</code>           \u2013            <p>The input dataframe containing features and the target variable.</p> </li> <li> <code>target</code>           \u2013            <p>The name of the target variable column in the dataframe.</p> </li> <li> <code>n_features</code>           \u2013            <p>The number of top features to select using feature selection methods.</p> </li> <li> <code>threshold</code>           \u2013            <p>The threshold for variance threshold feature selection. Default is 0.0.</p> </li> <li> <code>features</code>           \u2013            <p>A list of feature names to consider for selection. If not provided, all columns except the target column are used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>A dictionary where keys are the names of feature selection methods and values are the sets of selected features.</p> </li> </ul> <p>Examples: <pre><code>import pandas as pd\nfrom optialgo import feature_selection\nselected_features = feature_selection(dataframe=df, target=\"target\", n_features=10)\nprint(selected_features)\n# output :{\n    'all': ['feature1', 'feature2', ...],\n    'f_mutual': ['feature1', 'feature3', ...],\n    'f_anova': ['feature2', 'feature4', ...],\n    ...\n}\n</code></pre></p>"},{"location":"utils/feature_selection/#optialgo.feature_select_rfe","title":"feature_select_rfe","text":"<pre><code>feature_select_rfe(X: DataFrame, y: DataFrame, n_features: int, t: str)\n</code></pre> <p>Perform feature selection using Recursive Feature Elimination (RFE).</p> <p>This method selects the top <code>n_features</code> from the input dataframe <code>X</code> using Recursive Feature Elimination with a specified model type for either classification or regression tasks.</p> <p>Parameters:</p> <ul> <li> <code>X</code>           \u2013            <p>The input dataframe containing feature values.</p> </li> <li> <code>y</code>           \u2013            <p>The target values corresponding to the input features.</p> </li> <li> <code>n_features</code>           \u2013            <p>The number of top features to select.</p> </li> <li> <code>t</code>           \u2013            <p>The type of task: \"classification\" or \"regression\". Determines which model to use for RFE.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code>          \u2013            <p>A list of the names of the selected features.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>t</code> is neither \"classification\" nor \"regression\".</p> </li> </ul> <p>Examples: <pre><code>from sklearn.datasets import load_iris\nfrom optialgo import feature_select_rfe\nimport pandas as pd\ndata = load_iris()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.DataFrame(data.target, columns=[\"target\"])\nselected_features = feature_select_rfe(X, y, n_features=2, t=\"classification\")\nprint(selected_features)\n# output : ['petal width (cm)', 'petal length (cm)']\n</code></pre></p>"},{"location":"utils/feature_selection/#optialgo.feature_select_anova","title":"feature_select_anova","text":"<pre><code>feature_select_anova(X: DataFrame, y: DataFrame, n_features: int)\n</code></pre> <p>Perform feature selection using ANOVA F-test.</p> <p>This method selects the top <code>n_features</code> features from the input dataframe <code>X</code> based on the ANOVA F-test. It uses the <code>SelectKBest</code> class with the <code>f_classif</code> scoring function to rank features by their importance.</p> <p>Parameters:</p> <ul> <li> <code>X</code>           \u2013            <p>The input dataframe containing feature values.</p> </li> <li> <code>y</code>           \u2013            <p>The target values corresponding to the input features.</p> </li> <li> <code>n_features</code>           \u2013            <p>The number of top features to select.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code>          \u2013            <p>A list of the names of the selected features.</p> </li> </ul> <p>Examples: <pre><code>from sklearn.datasets import load_iris\nfrom optialgo import feature_select_anova\nimport pandas as pd\ndata = load_iris()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.DataFrame(data.target, columns=[\"target\"])\nselected_features = feature_select_anova(X, y, n_features=2)\nprint(selected_features)\n# output : ['petal length (cm)', 'petal width (cm)']\n</code></pre></p>"},{"location":"utils/feature_selection/#optialgo.feature_select_chi_square","title":"feature_select_chi_square","text":"<pre><code>feature_select_chi_square(X: DataFrame, y: DataFrame, n_features: int) -&gt; list\n</code></pre> <p>Selects the top <code>n_features</code> based on the chi-square statistic for classification tasks.</p> <p>This function evaluates the importance of categorical features in a dataset using the chi-square statistical test. It selects the most important features according to the specified number of features to select.</p> <p>Parameters:</p> <ul> <li> <code>X</code>           \u2013            <p>The input dataframe containing the feature columns.</p> </li> <li> <code>y</code>           \u2013            <p>The target variable column.</p> </li> <li> <code>n_features</code>           \u2013            <p>The number of top features to select based on the chi-square statistic.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>A list of the selected feature names ranked by their chi-square scores.</p> </li> </ul> <p>Examples: <pre><code>import pandas as pd\nfrom sklearn.datasets import load_iris\nfrom optialgo import feature_select_chi_square\nfrom sklearn.model_selection import train_test_split\ndata = load_iris()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.Series(data.target)\nselected_features = feature_select_chi_square(X, y, n_features=2)\nprint(selected_features)\n# output : ['petal length (cm)', 'petal width (cm)']\n</code></pre></p>"},{"location":"utils/feature_selection/#optialgo.feature_select_fisher_score","title":"feature_select_fisher_score","text":"<pre><code>feature_select_fisher_score(X: DataFrame, y: DataFrame, n_features: int) -&gt; list\n</code></pre> <p>Perform feature selection using Fisher Score.</p> <p>This method selects the top <code>n_features</code> from the input dataframe <code>X</code> based on the Fisher Score. Fisher Score is a supervised feature selection method that evaluates the importance of a feature by measuring the discriminative power of each feature with respect to the target <code>y</code>.</p> <p>Parameters:</p> <ul> <li> <code>X</code>           \u2013            <p>The input dataframe containing feature values.</p> </li> <li> <code>y</code>           \u2013            <p>The target values corresponding to the input features.</p> </li> <li> <code>n_features</code>           \u2013            <p>The number of top features to select.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>A list of the names of the selected features.</p> </li> </ul> <p>Examples <pre><code>from sklearn.datasets import load_iris\nfrom optialgo import feature_select_fisher_score\nimport pandas as pd\ndata = load_iris()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.DataFrame(data.target, columns=[\"target\"])\nselected_features = feature_select_fisher_score(X, y, n_features=2)\nprint(selected_features)\n# output :['petal width (cm)', 'petal length (cm)']\n</code></pre></p>"},{"location":"utils/feature_selection/#optialgo.feature_select_variance_threshold","title":"feature_select_variance_threshold","text":"<pre><code>feature_select_variance_threshold(X: DataFrame, threshold: int) -&gt; list\n</code></pre> <p>Perform feature selection using Variance Threshold.</p> <p>This method selects features from the input dataframe <code>X</code> based on a variance threshold. Features with a variance lower than the threshold will be removed, as they are less likely to be informative.</p> <p>Parameters:</p> <ul> <li> <code>X</code>           \u2013            <p>The input dataframe containing feature values.</p> </li> <li> <code>threshold</code>           \u2013            <p>The variance threshold. Features with a variance lower than this value will be removed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>A list of the names of the selected features that have a variance above the threshold.</p> </li> </ul> <p>Examples: <pre><code>from sklearn.datasets import load_iris\nfrom optialgo import feature_select_variance_threshold\nimport pandas as pd\ndata = load_iris()\nX = pd.DataFrame(data.data, columns=data.feature_names)\nselected_features = feature_select_variance_threshold(X, threshold=0.5)\nprint(selected_features)\n# output : ['petal length (cm)', 'petal width (cm)']\n</code></pre></p>"},{"location":"utils/feature_selection/#optialgo.feature_select_information_gain","title":"feature_select_information_gain","text":"<pre><code>feature_select_information_gain(X: DataFrame, y: DataFrame, n_features: int, t: str) -&gt; list\n</code></pre> <p>Selects the top <code>n_features</code> based on information gain for classification or regression tasks.</p> <p>This function evaluates the importance of features in a dataset using mutual information. It supports both classification and regression tasks, and selects the most important features according to the specified number of features to select.</p> <p>Parameters:</p> <ul> <li> <code>X</code>           \u2013            <p>The input dataframe containing the feature columns.</p> </li> <li> <code>y</code>           \u2013            <p>The target variable column.</p> </li> <li> <code>n_features</code>           \u2013            <p>The number of top features to select based on information gain.</p> </li> <li> <code>t</code>           \u2013            <p>The type of task. It can be either \"classification\" or \"regression\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>A list of the selected feature names ranked by their importance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>t</code> is not \"classification\" or \"regression\".</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from optialgo import feature_select_information_gain\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; data = load_iris()\n&gt;&gt;&gt; X = pd.DataFrame(data.data, columns=data.feature_names)\n&gt;&gt;&gt; y = pd.Series(data.target)\n&gt;&gt;&gt; selected_features = feature_select_information_gain(X, y, n_features=2, t='classification')\n&gt;&gt;&gt; print(selected_features)\n['petal length (cm)', 'petal width (cm)']\n</code></pre>"},{"location":"utils/handle_missing_values/","title":"Handle Missing Values","text":""},{"location":"utils/handle_missing_values/#optialgo.handling_missing_values","title":"handling_missing_values","text":"<pre><code>handling_missing_values(dataframe: DataFrame, imputation: dict = None, threshold: float = 0.3) -&gt; DataFrame\n</code></pre> <p>Handle missing values in a DataFrame by either imputing or dropping columns based on a threshold.</p> <p>This function handles missing values in the given DataFrame. It can perform custom imputation using the provided dictionary of imputers, drop columns with missing values exceeding a specified threshold, and impute the remaining missing values using median for numerical columns and most frequent value for categorical columns.</p> <p>Args: dataframe : The input DataFrame containing missing values. imputation : A dictionary where keys are column names and values are imputer instances from sklearn.impute. If provided, these imputers will be used to fill missing values in the specified columns. threshold : A float value between 0 and 1 that specifies the maximum allowable fraction of missing values in a column.  Columns with a fraction of missing values greater than or equal to this threshold will be dropped.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: A DataFrame with missing values handled according to the specified parameters.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the threshold is greater than or equal to 1.</p> </li> </ul> <p>Examples <pre><code>import pandas as pd\nfrom optialgo import handling_missing_values\nfrom sklearn.impute import SimpleImputer\ndata = {'A': [1, 2, None, 4], 'B': [None, 2, 3, 4], 'C': [1, 2, 3, None]}\ndf = pd.DataFrame(data)\nimputer = SimpleImputer(strategy='mean')\nhandled_df = handling_missing_values(df, imputation={'A': imputer}, threshold=0.5)\n</code></pre></p> <p>Notes: <pre><code>1. This function creates a copy of the input DataFrame to avoid modifying the original DataFrame.\n2. Columns with missing values exceeding the specified threshold are dropped.\n3. For remaining columns with missing values, numerical columns are imputed using the median, and categorical columns are imputed using the most frequent value.\n</code></pre></p>"},{"location":"utils/sampling/","title":"Sampling","text":""},{"location":"utils/sampling/#optialgo.sampling","title":"sampling","text":"<pre><code>sampling(dataframe: DataFrame, features: list, target: str, method: Literal['smote', 'over', 'under'] = 'smote', sampling_strategy: str = 'auto', seed: int = 42)\n</code></pre> <p>Applies sampling techniques to balance the target classes in the dataframe.</p> <p>Parameters:</p> <ul> <li> <code>dataframe</code>           \u2013            <p>The input dataframe containing the features and target column.</p> </li> <li> <code>features</code>           \u2013            <p>A list of feature column names to be used for sampling.</p> </li> <li> <code>target</code>           \u2013            <p>The name of the target column to be balanced.</p> </li> <li> <code>method</code>           \u2013            <p>The sampling method to be used. </p> </li> <li> <code>sampling_strategy</code>           \u2013            <p>The sampling strategy to use. Default is \"auto\".</p> </li> <li> <code>seed</code>           \u2013            <p>The random seed for reproducibility. Default is 42.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>pd.DataFrame: The dataframe with the target classes balanced according to the specified method.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an invalid sampling method is provided.</p> </li> </ul> <p>Examples: <pre><code>import pandas as pd\nfrom optialgo import sampling\ndf = pd.DataFrame({\n    'feature1': [1, 2, 3, 4, 5, 6],\n    'feature2': [7, 8, 9, 10, 11, 12],\n    'target': [0, 0, 0, 1, 1, 1]\n})\nsampled_df = sampling(df, features=['feature1', 'feature2'], target='target', method='over')\nsampled_df['target'].value_counts()\n</code></pre></p>"},{"location":"utils/text_preprocessing/","title":"Text Preprocessing","text":""},{"location":"utils/text_preprocessing/#optialgo.f_remove_punctuation","title":"f_remove_punctuation  <code>cached</code>","text":"<pre><code>f_remove_punctuation(text: str, punctuation: str = string.punctuation) -&gt; str\n</code></pre> <p>Remove punctuation from a single text string.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>The input text from which to remove punctuation.</p> </li> <li> <code>punctuation</code>           \u2013            <p>The punctuation characters to remove, by default string.punctuation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The text with punctuation removed.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.remove_punctuation","title":"remove_punctuation","text":"<pre><code>remove_punctuation(data: List[str], punctuation: str = string.punctuation) -&gt; List[str]\n</code></pre> <p>Remove punctuation from a list of texts.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of texts.</p> </li> <li> <code>punctuation</code>           \u2013            <p>The punctuation characters to remove, by default string.punctuation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str] : List of texts without punctuation.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.f_remove_digits","title":"f_remove_digits","text":"<pre><code>f_remove_digits(text: str) -&gt; str\n</code></pre> <p>Remove digits from a single text string.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>The input text from which to remove digits.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The text with digits removed.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.remove_digits","title":"remove_digits","text":"<pre><code>remove_digits(data: List[str]) -&gt; List[str]\n</code></pre> <p>Remove digits from a list of texts.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of texts.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of texts without digits.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.f_remove_url","title":"f_remove_url  <code>cached</code>","text":"<pre><code>f_remove_url(text) -&gt; str\n</code></pre> <p>Remove URLs from a single text string.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>The input text from which to remove URLs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The text with URLs removed.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.remove_url","title":"remove_url","text":"<pre><code>remove_url(data: List[str]) -&gt; List[str]\n</code></pre> <p>Remove URLs from a list of texts.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of texts.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of texts without URLs.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.f_remove_emoji","title":"f_remove_emoji  <code>cached</code>","text":"<pre><code>f_remove_emoji(text: str) -&gt; str\n</code></pre> <p>Remove emojis from a single text string.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>The input text from which to remove emojis.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The text with emojis removed.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.remove_emoji","title":"remove_emoji","text":"<pre><code>remove_emoji(data: List[str]) -&gt; List[str]\n</code></pre> <p>Remove emojis from a list of texts.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of texts.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of texts without emojis.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.f_remove_non_latin","title":"f_remove_non_latin  <code>cached</code>","text":"<pre><code>f_remove_non_latin(text: str) -&gt; str\n</code></pre> <p>Remove non-Latin characters from a single text string.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>The input text from which to remove non-Latin characters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The text with non-Latin characters removed.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.remove_non_latin","title":"remove_non_latin","text":"<pre><code>remove_non_latin(data: List[str]) -&gt; List[str]\n</code></pre> <p>Remove non-Latin characters from a list of texts.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of texts.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of texts without non-Latin characters.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.F_TEXT_CLEAN","title":"F_TEXT_CLEAN  <code>module-attribute</code>","text":"<pre><code>F_TEXT_CLEAN = ['remove_punctuation', 'remove_digits', 'remove_url', 'remove_emoji', 'remove_non_latin']\n</code></pre>"},{"location":"utils/text_preprocessing/#optialgo.lower_text","title":"lower_text","text":"<pre><code>lower_text(data: List[str]) -&gt; List[str]\n</code></pre>"},{"location":"utils/text_preprocessing/#optialgo.f_regex_word_tokenize","title":"f_regex_word_tokenize","text":"<pre><code>f_regex_word_tokenize(text: str, pattern: Pattern[str]) -&gt; Tuple[str]\n</code></pre> <p>Tokenize a text string using a regex pattern.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>The input text to tokenize.</p> </li> <li> <code>pattern</code>           \u2013            <p>The regex pattern to use for tokenization.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[str]</code>           \u2013            <p>Tuple[str]: A tuple of tokens.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.f_word_tokenize","title":"f_word_tokenize","text":"<pre><code>f_word_tokenize(text: str)\n</code></pre> <p>Tokenize a text string using NLTK's word_tokenize.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>The input text to tokenize.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Tuple[str]: A tuple of tokens.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.word_tokenize","title":"word_tokenize","text":"<pre><code>word_tokenize(data: List[str], pattern: Optional[Pattern[str]] = None) -&gt; List[Tuple[str, ...]]\n</code></pre> <p>Tokenize a list of text strings.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of texts to tokenize.</p> </li> <li> <code>pattern</code>           \u2013            <p>The regex pattern to use for tokenization, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Tuple[str, ...]]</code>           \u2013            <p>List[Tuple[str, ...]]: List of tuples of tokens.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.token_to_str","title":"token_to_str","text":"<pre><code>token_to_str(data: List[Tuple[str]]) -&gt; List[str]\n</code></pre> <p>Convert a list of token tuples to a list of strings.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of token tuples.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of strings joined from tokens.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.find_duplicates","title":"find_duplicates","text":"<pre><code>find_duplicates(data: List[str]) -&gt; dict\n</code></pre> <p>Find duplicates in a list of strings and return their indices.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of strings to check for duplicates.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>A dictionary where keys are duplicate strings and values are lists of their indices.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.text_clean","title":"text_clean","text":"<pre><code>text_clean(data: List[str], punctuation: Optional[str] = string.punctuation, lower: bool = True, digits: bool = True, emoji: bool = True, duplicates: bool = True, url: bool = True, non_latin: bool = True, return_token: bool = False, return_dataframe: bool = False, verbose: bool = False, pattern: Optional[Pattern[str]] = None)\n</code></pre> <p>Clean a list of text data based on specified parameters.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of strings to clean.</p> </li> <li> <code>punctuation</code>           \u2013            <p>Punctuation characters to remove, by default string.punctuation.</p> </li> <li> <code>lower</code>           \u2013            <p>Convert text to lowercase, by default True.</p> </li> <li> <code>digits</code>           \u2013            <p>Remove digits from text, by default True.</p> </li> <li> <code>emoji</code>           \u2013            <p>Remove emojis from text, by default True.</p> </li> <li> <code>duplicates</code>           \u2013            <p>Remove duplicate entries from data, by default True.</p> </li> <li> <code>url</code>           \u2013            <p>Remove URLs from text, by default True.</p> </li> <li> <code>non_latin</code>           \u2013            <p>Remove non-Latin characters from text, by default True.</p> </li> <li> <code>return_token</code>           \u2013            <p>Tokenize text and return tokens, by default False.</p> </li> <li> <code>return_dataframe</code>           \u2013            <p>Return result as a Pandas DataFrame, by default False.</p> </li> <li> <code>verbose</code>           \u2013            <p>Display progress using tqdm, by default False.</p> </li> <li> <code>pattern</code>           \u2013            <p>Regex pattern for tokenization, by default None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>List[str]: Cleaned list of text data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If return_dataframe is True but lengths of data and cleaned data don't match.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.get_stopwords_en","title":"get_stopwords_en","text":"<pre><code>get_stopwords_en() -&gt; List[str]\n</code></pre> <p>Get English stopwords from NLTK corpus.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of English stopwords.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.get_stopwords_idn","title":"get_stopwords_idn","text":"<pre><code>get_stopwords_idn() -&gt; List[str]\n</code></pre> <p>Get Indonesian stopwords from NLTK corpus.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str] List of Indonesian stopwords.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.f_stopwords","title":"f_stopwords","text":"<pre><code>f_stopwords(token: Union[Tuple[str], str], stopwords: Set[str], return_token: bool = False) -&gt; Union[Tuple[str, ...], str]\n</code></pre> <p>Remove stopwords from a token or tuple of tokens.</p> <p>Parameters:</p> <ul> <li> <code>token</code>           \u2013            <p>Token or tuple of tokens to filter.</p> </li> <li> <code>stopwords</code>           \u2013            <p>Set of stopwords to filter out.</p> </li> <li> <code>return_token</code>           \u2013            <p>Whether to return tokens (True) or a joined string (False), by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tuple[str, ...], str]</code>           \u2013            <p>Union[Tuple[str, ...], str]: Filtered tokens or joined string without stopwords.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.remove_stopwords","title":"remove_stopwords","text":"<pre><code>remove_stopwords(tokens: List[Tuple[str]], lang: str, stopwords: Optional[List[str]] = None, additional: Optional[List[str]] = None, return_token: bool = True, verbose: bool = False) -&gt; Union[List[str], List[Tuple[str]]]\n</code></pre> <p>Remove stopwords from a list of tokenized texts.</p> <p>Parameters:</p> <ul> <li> <code>tokens</code>           \u2013            <p>List of tokenized texts, where each item can be a tuple or list of tokens.</p> </li> <li> <code>lang</code>           \u2013            <p>Language code for stopwords ('english' or 'indonesian').</p> </li> <li> <code>stopwords</code>           \u2013            <p>List of additional stopwords to remove, by default None.</p> </li> <li> <code>additional</code>           \u2013            <p>List of additional stopwords to add, by default None.</p> </li> <li> <code>return_token</code>           \u2013            <p>Whether to return tokens (True) or joined strings (False), by default True.</p> </li> <li> <code>verbose</code>           \u2013            <p>Whether to display progress bar, by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[List[str], List[Tuple[str]]]</code>           \u2013            <p>Union[List[str], List[Tuple[str]]]: List of tokenized texts with stopwords removed.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.f_normalize","title":"f_normalize","text":"<pre><code>f_normalize(token: Union[Tuple[str], str], norm_words: dict, return_token: bool = False) -&gt; Union[Tuple[str, ...], str]\n</code></pre> <p>Normalize tokens using a dictionary of normalization mappings.</p> <p>Parameters:</p> <ul> <li> <code>token</code>           \u2013            <p>Token or tuple of tokens to normalize.</p> </li> <li> <code>norm_words</code>           \u2013            <p>Dictionary mapping tokens to their normalized forms.</p> </li> <li> <code>return_token</code>           \u2013            <p>Whether to return tokens (True) or a joined string (False), by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tuple[str, ...], str]</code>           \u2013            <p>Union[Tuple[str, ...], str]: Normalized tokens or joined string of normalized tokens.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.f_lemmatization_en","title":"f_lemmatization_en","text":"<pre><code>f_lemmatization_en(token: Union[Tuple[str], str], return_token: bool = False) -&gt; Union[Tuple[str, ...], str]\n</code></pre> <p>Lemmatize English tokens based on their part-of-speech tags using a lemmatizer.</p> <p>Parameters:</p> <ul> <li> <code>token</code>           \u2013            <p>Token or tuple of tokens to lemmatize.</p> </li> <li> <code>return_token</code>           \u2013            <p>Whether to return tokens (True) or joined string (False), by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tuple[str, ...], str]</code>           \u2013            <p>Union[Tuple[str, ...], str]: If return_token is True, returns a tuple of lemmatized tokens. If return_token is False, returns the lemmatized text as a joined string.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.f_lemmatization_idn","title":"f_lemmatization_idn","text":"<pre><code>f_lemmatization_idn(text: str, return_token: bool = False) -&gt; Union[Tuple]\n</code></pre> <p>Lemmatize Indonesian text.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>Input text to lemmatize.</p> </li> <li> <code>return_token</code>           \u2013            <p>Whether to return tokens (True) or joined string (False), by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tuple]</code>           \u2013            <p>Union[Tuple[str], str]: If return_token is True, returns a tuple of lemmatized tokens. If return_token is False, returns the lemmatized text as a string.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.lemmatization","title":"lemmatization","text":"<pre><code>lemmatization(data: Union[List[Tuple[str]], List[str]], lang: str, return_token: bool = False, verbose: bool = False)\n</code></pre> <p>Lemmatize tokenized texts based on language.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of tokenized texts, where each item can be a tuple or list of tokens.</p> </li> <li> <code>lang</code>           \u2013            <p>Language code for lemmatization ('indonesian' or 'english').</p> </li> <li> <code>return_token</code>           \u2013            <p>Whether to return tokens (True) or joined string (False), by default False.</p> </li> <li> <code>verbose</code>           \u2013            <p>Whether to display progress bar, by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>List[Union[Tuple[str], str]]: List of lemmatized texts. If return_token is True, each item is a tuple of lemmatized tokens. If return_token is False, each item is a joined string of lemmatized tokens.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.f_stemming_en","title":"f_stemming_en","text":"<pre><code>f_stemming_en(text: str, return_token: bool = False) -&gt; Union[Tuple[str, ...], str]\n</code></pre> <p>Perform stemming on English text.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>Input text to stem.</p> </li> <li> <code>return_token</code>           \u2013            <p>Whether to return tokens (True) or joined string (False), by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tuple[str, ...], str]</code>           \u2013            <p>Union[Tuple[str, ...], str]: If return_token is True, returns a tuple of stemmed tokens. If return_token is False, returns the stemmed text as a joined string.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.f_stemming_idn","title":"f_stemming_idn","text":"<pre><code>f_stemming_idn(text: str, return_token: bool = False) -&gt; Union[Tuple[str, ...], str]\n</code></pre> <p>Perform stemming on Indonesian text.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>Input text to stem.</p> </li> <li> <code>return_token</code>           \u2013            <p>Whether to return tokens (True) or joined string (False), by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tuple[str, ...], str]</code>           \u2013            <p>Union[Tuple[str, ...], str]: If return_token is True, returns a tuple of stemmed tokens. If return_token is False, returns the stemmed text as a joined string.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.stemming","title":"stemming","text":"<pre><code>stemming(data: List[Tuple[str]], lang: str, return_token: bool = False, verbose: bool = False)\n</code></pre> <p>Perform stemming on tokenized texts based on language.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of tokenized texts, where each item is a tuple of tokens.</p> </li> <li> <code>lang</code>           \u2013            <p>Language code for stemming ('indonesian' or 'english').</p> </li> <li> <code>return_token</code>           \u2013            <p>Whether to return tokens (True) or joined string (False), by default False.</p> </li> <li> <code>verbose</code>           \u2013            <p>Whether to display a progress bar, by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>List[str]: List of stemmed texts. If return_token is True, each item is a list of stemmed tokens. If return_token is False, each item is a joined string of stemmed tokens.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.text_manipulation","title":"text_manipulation","text":"<pre><code>text_manipulation(tokens: List[Tuple[str]], lang: str, stopwords: Union[List[str], bool] = False, stem: bool = False, return_dataframe: bool = False, norm_words: Optional[dict] = None, return_token=False, additional: Optional[List[str]] = None, verbose: bool = False)\n</code></pre> <p>Perform text manipulation including normalization, stopword removal, stemming or lemmatization.</p> <p>Parameters:</p> <ul> <li> <code>tokens</code>           \u2013            <p>List of tokenized texts, where each item is a tuple of tokens.</p> </li> <li> <code>lang</code>           \u2013            <p>Language code for text manipulation ('indonesian' or 'english').</p> </li> <li> <code>stopwords</code>           \u2013            <p>List of stopwords or True to use default stopwords for the specified language, by default False.</p> </li> <li> <code>stem</code>           \u2013            <p>Whether to perform stemming (True) or lemmatization (False), by default False.</p> </li> <li> <code>return_dataframe</code>           \u2013            <p>Whether to return results as a DataFrame, by default False.</p> </li> <li> <code>norm_words</code>           \u2013            <p>Dictionary of normalization words, by default None.</p> </li> <li> <code>return_token</code>           \u2013            <p>Whether to return tokens (True) or joined string (False), by default False.</p> </li> <li> <code>additional</code>           \u2013            <p>Additional stopwords to include, by default None.</p> </li> <li> <code>verbose</code>           \u2013            <p>Whether to display progress bars for preprocessing steps, by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Union[List[str], pd.DataFrame]: If return_dataframe is False, returns a list of preprocessed texts/tokens. If return_dataframe is True, returns a pandas DataFrame with columns \"raw\" and \"pre\".</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.Tokenizer","title":"Tokenizer","text":"<pre><code>Tokenizer(oov_token: str = None, filters: Optional[str] = string.punctuation, min_count: Optional[int] = None, maxlen: int = 0, padding: Literal['pre', 'post'] = 'pre', truncating: Literal['pre', 'post'] = 'pre', value: int = 0, dtype='int32')\n</code></pre> <p>A class to tokenize text data, transform it into sequences, and pad sequences.</p> <p>Attributes:</p> <ul> <li> <code>min_count</code>           \u2013            <p>Minimum frequency count for words to be included in the vocabulary.</p> </li> <li> <code>filters</code>           \u2013            <p>Characters to filter out from the text.</p> </li> <li> <code>oov_token</code>           \u2013            <p>Token to use for out-of-vocabulary words.</p> </li> <li> <code>maxlen</code>           \u2013            <p>Maximum length of sequences. If 0, it will be calculated based on the data.</p> </li> <li> <code>padding</code>           \u2013            <p>Padding type (\"pre\" or \"post\").</p> </li> <li> <code>truncating</code>           \u2013            <p>Truncating type (\"pre\" or \"post\").</p> </li> <li> <code>value</code>           \u2013            <p>Value used for padding.</p> </li> <li> <code>dtype</code>           \u2013            <p>Data type of the padded sequences.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>fit</code>             \u2013              <p>Union[List[str], List[Tuple[str]]], y=None) Fits the tokenizer on the given data.</p> </li> <li> <code>encode</code>             \u2013              <p>str) -&gt; list Encodes a single text string into a sequence of integers.</p> </li> <li> <code>decode</code>             \u2013              <p>list) -&gt; str Decodes a sequence of integers back into a text string.</p> </li> <li> <code>texts_to_sequences</code>             \u2013              <p>list) -&gt; list Converts a list of text strings into sequences of integers.</p> </li> <li> <code>sequences_to_texts</code>             \u2013              <p>list) -&gt; list Converts a list of sequences of integers back into text strings.</p> </li> <li> <code>sequences_to_pad</code>             \u2013              <p>list) Pads a list of sequences to the maximum length.</p> </li> <li> <code>texts_to_pad_sequences</code>             \u2013              <p>list) Converts a list of text strings into padded sequences.</p> </li> </ul> Properties <p>word_index : A dictionary mapping words to their integer indices. index_word : A dictionary mapping integer indices to their corresponding words.</p> <p>Parameters:</p> <ul> <li> <code>oov_token</code>           \u2013            <p>Token to use for out-of-vocabulary words (default is None).</p> </li> <li> <code>filters</code>           \u2013            <p>Characters to filter out from the text (default is string.punctuation).</p> </li> <li> <code>min_count</code>           \u2013            <p>Minimum frequency count for words to be included in the vocabulary (default is None).</p> </li> <li> <code>maxlen</code>           \u2013            <p>Maximum length of sequences (default is 0, which means it will be calculated based on the data).</p> </li> <li> <code>padding</code>           \u2013            <p>Padding type (\"pre\" or \"post\", default is \"pre\").</p> </li> <li> <code>truncating</code>           \u2013            <p>Truncating type (\"pre\" or \"post\", default is \"pre\").</p> </li> <li> <code>value</code>           \u2013            <p>Value used for padding (default is 0).</p> </li> <li> <code>dtype</code>           \u2013            <p>Data type of the padded sequences (default is \"int32\").</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.Tokenizer.index_word","title":"index_word  <code>property</code>","text":"<pre><code>index_word\n</code></pre> <p>Returns the index-to-word dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>The index-to-word dictionary.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.Tokenizer.word_index","title":"word_index  <code>property</code>","text":"<pre><code>word_index\n</code></pre> <p>Returns the word-to-index dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>The word-to-index dictionary.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.Tokenizer.decode","title":"decode","text":"<pre><code>decode(token: list) -&gt; str\n</code></pre> <p>Decodes a sequence of integers back into a text string.</p> <p>Parameters:</p> <ul> <li> <code>token</code>           \u2013            <p>The sequence of integers to decode.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The decoded text string.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.Tokenizer.encode","title":"encode","text":"<pre><code>encode(text: str) -&gt; list\n</code></pre> <p>Encodes a single text string into a sequence of integers.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>The text string to encode.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>The encoded sequence of integers.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.Tokenizer.fit","title":"fit","text":"<pre><code>fit(data: Union[List[str], List[Tuple[str]]], y=None)\n</code></pre> <p>Fits the tokenizer on the given data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>The input data to fit the tokenizer on.</p> </li> <li> <code>y</code>           \u2013            <p>Not used, present for compatibility with sklearn API.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tokenizer</code>          \u2013            <p>The fitted Tokenizer object.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.Tokenizer.sequences_to_pad","title":"sequences_to_pad","text":"<pre><code>sequences_to_pad(sequences: list)\n</code></pre> <p>Pads a list of sequences to the maximum length.</p> <p>Parameters:</p> <ul> <li> <code>sequences</code>           \u2013            <p>The list of sequences to pad.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>np.ndarray: The padded sequences.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.Tokenizer.sequences_to_texts","title":"sequences_to_texts","text":"<pre><code>sequences_to_texts(sequences: list) -&gt; list\n</code></pre> <p>Converts a list of sequences of integers back into text strings.</p> <p>Parameters:</p> <ul> <li> <code>sequences</code>           \u2013            <p>The list of sequences of integers to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>The list of decoded text strings.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.Tokenizer.texts_to_pad_sequences","title":"texts_to_pad_sequences","text":"<pre><code>texts_to_pad_sequences(text: list)\n</code></pre> <p>Converts a list of text strings into padded sequences.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>The list of text strings to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>np.ndarray: The padded sequences.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.Tokenizer.texts_to_sequences","title":"texts_to_sequences","text":"<pre><code>texts_to_sequences(text: list) -&gt; list\n</code></pre> <p>Converts a list of text strings into sequences of integers.</p> <p>Parameters:</p> <ul> <li> <code>text</code>           \u2013            <p>The list of text strings to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list</code> )          \u2013            <p>The list of sequences of integers.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.Tokenizer.transform","title":"transform","text":"<pre><code>transform(data)\n</code></pre> <p>Transforms the given data into padded sequences.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>The input data to transform.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>np.ndarray: The transformed and padded sequences.</p> </li> </ul>"},{"location":"utils/text_preprocessing/#optialgo.count_words","title":"count_words","text":"<pre><code>count_words(data: Union[List[str], List[Tuple[str]]], min_count: Optional[int] = None, return_dataframe: bool = False)\n</code></pre> <p>Count the occurrences of words in a list of tokenized texts or strings.</p> <p>Parameters:</p> <ul> <li> <code>data</code>           \u2013            <p>List of tokenized texts (list of tuples or list of strings).</p> </li> <li> <code>min_count</code>           \u2013            <p>Minimum count threshold for words to be included in the result, by default None.</p> </li> <li> <code>return_dataframe</code>           \u2013            <p>Whether to return the result as a pandas DataFrame, by default False.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Union[dict, pd.DataFrame]: If return_dataframe is False, returns a dictionary where keys are words and values are counts. If return_dataframe is True, returns a DataFrame with columns \"words\" and \"counts\".</p> </li> </ul>"}]}